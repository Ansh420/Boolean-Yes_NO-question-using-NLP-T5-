{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-13T02:27:58.266925Z","iopub.execute_input":"2023-03-13T02:27:58.267349Z","iopub.status.idle":"2023-03-13T02:27:58.274788Z","shell.execute_reply.started":"2023-03-13T02:27:58.267311Z","shell.execute_reply":"2023-03-13T02:27:58.273258Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import time\nimport torch\nfrom transformers import T5ForConditionalGeneration,T5Tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-03-13T02:27:58.277799Z","iopub.execute_input":"2023-03-13T02:27:58.278440Z","iopub.status.idle":"2023-03-13T02:28:12.727078Z","shell.execute_reply.started":"2023-03-13T02:27:58.278382Z","shell.execute_reply":"2023-03-13T02:28:12.725599Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed):\n  torch.manual_seed(seed)\n  if torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-13T02:28:12.729104Z","iopub.execute_input":"2023-03-13T02:28:12.729583Z","iopub.status.idle":"2023-03-13T02:28:12.738610Z","shell.execute_reply.started":"2023-03-13T02:28:12.729534Z","shell.execute_reply":"2023-03-13T02:28:12.737465Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_boolean_questions')\ntokenizer = T5Tokenizer.from_pretrained('t5-base')","metadata":{"execution":{"iopub.status.busy":"2023-03-13T02:28:12.740358Z","iopub.execute_input":"2023-03-13T02:28:12.740766Z","iopub.status.idle":"2023-03-13T02:28:36.205466Z","shell.execute_reply.started":"2023-03-13T02:28:12.740728Z","shell.execute_reply":"2023-03-13T02:28:36.204046Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc082416fe384766ad22e1d43aa2bfcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09bc9091ac2f4c9eb14fc26c1d3a53a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"630cadae4a7a49b7a349ff3067e7a409"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db14cfa48dce46109c4511b866488eea"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/models/t5/tokenization_t5.py:173: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\nFor now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print (\"device \",device)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T02:28:36.209701Z","iopub.execute_input":"2023-03-13T02:28:36.210340Z","iopub.status.idle":"2023-03-13T02:28:36.227828Z","shell.execute_reply.started":"2023-03-13T02:28:36.210295Z","shell.execute_reply":"2023-03-13T02:28:36.226131Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def greedy_decoding (inp_ids,attn_mask):\n  greedy_output = model.generate(input_ids=inp_ids, attention_mask=attn_mask, max_length=500)\n  Question =  tokenizer.decode(greedy_output[0], skip_special_tokens=True,clean_up_tokenization_spaces=True)\n  return Question.strip().capitalize()","metadata":{"execution":{"iopub.status.busy":"2023-03-13T02:28:36.230804Z","iopub.execute_input":"2023-03-13T02:28:36.232299Z","iopub.status.idle":"2023-03-13T02:28:39.033624Z","shell.execute_reply.started":"2023-03-13T02:28:36.232243Z","shell.execute_reply":"2023-03-13T02:28:39.032334Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def beam_search_decoding (inp_ids,attn_mask):\n  beam_output = model.generate(input_ids=inp_ids,\n                                 attention_mask=attn_mask,\n                                 max_length=500,\n                               num_beams=10,\n                               num_return_sequences=3,\n                               no_repeat_ngram_size=2,\n                               early_stopping=True\n                               )\n  Questions = [tokenizer.decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True) for out in\n               beam_output]\n  return [Question.strip().capitalize() for Question in Questions]\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-13T02:28:39.035723Z","iopub.execute_input":"2023-03-13T02:28:39.037082Z","iopub.status.idle":"2023-03-13T02:28:39.147380Z","shell.execute_reply.started":"2023-03-13T02:28:39.036988Z","shell.execute_reply":"2023-03-13T02:28:39.146276Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def topkp_decoding (inp_ids,attn_mask):\n  topkp_output = model.generate(input_ids=inp_ids,\n                                 attention_mask=attn_mask,\n                                 max_length=256,\n                               do_sample=True,\n                               top_k=40,\n                               top_p=0.80,\n                               num_return_sequences=3,\n                                no_repeat_ngram_size=2,\n                                early_stopping=True\n                               )\n  Questions = [tokenizer.decode(out, skip_special_tokens=True,clean_up_tokenization_spaces=True) for out in topkp_output]\n  return [Question.strip().capitalize() for Question in Questions]","metadata":{"execution":{"iopub.status.busy":"2023-03-13T02:28:39.149639Z","iopub.execute_input":"2023-03-13T02:28:39.150592Z","iopub.status.idle":"2023-03-13T02:28:39.229661Z","shell.execute_reply.started":"2023-03-13T02:28:39.150539Z","shell.execute_reply":"2023-03-13T02:28:39.227982Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"passage ='''A block has components that make it smarter than a classical neuron and a memory for recent sequences. A block contains gates that manage the block’s state and output. A block operates upon an input sequence, and each gate within a block uses the sigmoid activation units to control whether it is triggered or not, making the change of state and addition of information flowing through the block conditional.'''\ntruefalse =\"yes\"","metadata":{"execution":{"iopub.status.busy":"2023-03-13T02:28:39.235798Z","iopub.execute_input":"2023-03-13T02:28:39.236396Z","iopub.status.idle":"2023-03-13T02:28:39.298349Z","shell.execute_reply.started":"2023-03-13T02:28:39.236339Z","shell.execute_reply":"2023-03-13T02:28:39.295000Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"text = \"truefalse: %s passage: %s </s>\" % (passage, truefalse)\nmax_len = 500\n","metadata":{"execution":{"iopub.status.busy":"2023-03-13T02:28:39.300363Z","iopub.execute_input":"2023-03-13T02:28:39.301408Z","iopub.status.idle":"2023-03-13T02:28:39.382708Z","shell.execute_reply.started":"2023-03-13T02:28:39.301348Z","shell.execute_reply":"2023-03-13T02:28:39.381144Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"encoding = tokenizer.encode_plus(text, return_tensors=\"pt\")\ninput_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-13T02:28:39.385079Z","iopub.execute_input":"2023-03-13T02:28:39.386491Z","iopub.status.idle":"2023-03-13T02:28:39.464543Z","shell.execute_reply.started":"2023-03-13T02:28:39.386435Z","shell.execute_reply":"2023-03-13T02:28:39.463140Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/models/t5/tokenization_t5.py:227: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  f\"This sequence already has {self.eos_token}. In future versions this behavior may lead to duplicated\"\n","output_type":"stream"}]},{"cell_type":"code","source":"print (\"Context: \",passage)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T02:28:39.466821Z","iopub.execute_input":"2023-03-13T02:28:39.467790Z","iopub.status.idle":"2023-03-13T02:28:39.536647Z","shell.execute_reply.started":"2023-03-13T02:28:39.467723Z","shell.execute_reply":"2023-03-13T02:28:39.534241Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Context:  A block has components that make it smarter than a classical neuron and a memory for recent sequences. A block contains gates that manage the block’s state and output. A block operates upon an input sequence, and each gate within a block uses the sigmoid activation units to control whether it is triggered or not, making the change of state and addition of information flowing through the block conditional.\n","output_type":"stream"}]},{"cell_type":"code","source":"output = beam_search_decoding(input_ids,attention_masks)\nprint (\"\\nBeam decoding [Most accurate questions] ::\\n\")\nfor out in output:\n    print(out)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T02:28:39.539037Z","iopub.execute_input":"2023-03-13T02:28:39.539551Z","iopub.status.idle":"2023-03-13T02:28:42.459411Z","shell.execute_reply.started":"2023-03-13T02:28:39.539497Z","shell.execute_reply":"2023-03-13T02:28:42.457950Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"\nBeam decoding [Most accurate questions] ::\n\nIs the state of a block dependent on the input sequence?\nIs a block and its output the same thing?\nIs the state of a block dependent upon the input sequence?\n","output_type":"stream"}]},{"cell_type":"code","source":"output = topkp_decoding(input_ids,attention_masks)\nprint (\"\\nTopKP decoding [Not very accurate but more variety in questions] ::\\n\")\nfor out in output:\n    print (out)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T02:28:42.464374Z","iopub.execute_input":"2023-03-13T02:28:42.465037Z","iopub.status.idle":"2023-03-13T02:28:45.328517Z","shell.execute_reply.started":"2023-03-13T02:28:42.464979Z","shell.execute_reply":"2023-03-13T02:28:45.327153Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"\nTopKP decoding [Not very accurate but more variety in questions] ::\n\nDoes a block have sigmoid activation units?\nIs a block smarter than remembrance and memory?\nIs a block able to remember if input is given?\n","output_type":"stream"}]},{"cell_type":"code","source":"#!pip install streamlit\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-13T02:28:45.330213Z","iopub.execute_input":"2023-03-13T02:28:45.330664Z","iopub.status.idle":"2023-03-13T02:28:45.337273Z","shell.execute_reply.started":"2023-03-13T02:28:45.330613Z","shell.execute_reply":"2023-03-13T02:28:45.335614Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}